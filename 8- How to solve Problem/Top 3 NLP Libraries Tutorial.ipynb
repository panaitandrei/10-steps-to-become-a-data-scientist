{
  "cells": [
    {
      "metadata": {
        "_uuid": "1247188a5193a0bb99f176fea36dc594d283160e"
      },
      "cell_type": "markdown",
      "source": "# <div style=\"text-align: center\">Top 3 NLP Libraries Tutorial</div>\n\n### <div style=\"text-align: center\">Quite Practical and Far from any Theoretical Concepts</div>\n<img src='https://chengotto.com/wp-content/uploads/2018/02/images.duckduckgo2-600x338.jpg'>\n<div style=\"text-align:center\">last update: <b>11/22/2018</b></div>\n\n\n>###### you may  be interested have a look at it: [**The Data Scientist’s Toolbox Tutorial - 1**](https://www.kaggle.com/mjbahmani/the-data-scientist-s-toolbox-tutorial-1)\n\n\n---------------------------------------------------------------------\nyou can Fork and Run this kernel on Github:\n> ###### [ GitHub](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist)\n\n-------------------------------------------------------------------------------------------------------------\n\n **I hope you find this kernel helpful and some <font color=\"red\"><b>UPVOTES</b></font> would be very much appreciated**\n \n -----------"
    },
    {
      "metadata": {
        "_uuid": "0a45d14ee727bf2f88a7cd0ba5e6aa338977d10b"
      },
      "cell_type": "markdown",
      "source": " <a id=\"top\"></a> <br>\n## Notebook  Content\n1. [Introduction](#1)\n    1. [Import](#2)\n    1. [Version](#3)\n1. [NLTK](#38)\n    1. [Tokenizing sentences](#39)\n    1. [NLTK and arrays](#40)\n    1. [NLTK stop words](#41)\n    1. [NLTK – stemming](#42)\n    1. [NLTK speech tagging](#43)\n    1. [Natural Language Processing – prediction](#44)\n    1. [nlp prediction example](#45)\n1. [conclusion](#5)\n1. [References](#6)"
    },
    {
      "metadata": {
        "_uuid": "ec7344e7f2a1bafa9a44a518722fcd8ec47c374b"
      },
      "cell_type": "markdown",
      "source": "<a id=\"1\"></a> <br>\n# 1-Introduction\nThis Kernel is mostly for **beginners**, and of course, all **professionals** who think they need to review  their  knowledge.\nAlso, this is  the second version for (  [The Data Scientist’s Toolbox Tutorial - 1](https://www.kaggle.com/mjbahmani/the-data-scientist-s-toolbox-tutorial-1) ) and we will continue with other important packages in this kernel.keep following!"
    },
    {
      "metadata": {
        "_uuid": "4e28cde75726e3617dc80585626f7f8a1297a9e4"
      },
      "cell_type": "markdown",
      "source": "<a id=\"2\"></a> <br>\n##   1-1 Import"
    },
    {
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_kg_hide-input": true,
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\nfrom pandas import get_dummies\nimport plotly.graph_objs as go\nfrom sklearn import datasets\nfrom sklearn.svm import SVC\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport warnings\nimport sklearn\nimport scipy\nimport numpy\nimport json\nimport nltk\nimport sys\nimport csv\nimport os",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3c3c434ac82d771c5549c4f36d0e8e878489f252"
      },
      "cell_type": "markdown",
      "source": "<a id=\"3\"></a> <br>\n## 1-2 Version"
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "72fdff866b7cbe404867e82f9122e16fc33facf2",
        "trusted": true
      },
      "cell_type": "code",
      "source": "print('matplotlib: {}'.format(matplotlib.__version__))\nprint('scipy: {}'.format(scipy.__version__))\nprint('seaborn: {}'.format(sns.__version__))\nprint('pandas: {}'.format(pd.__version__))\nprint('numpy: {}'.format(np.__version__))\nprint('Python: {}'.format(sys.version))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4284a92f8326eb09dccf0a795f44931c5a7487cc"
      },
      "cell_type": "markdown",
      "source": "## 1-3 Setup\n\nA few tiny adjustments for better **code readability**"
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "f9a265dce077fd183b2172378a85ed2d23290189",
        "trusted": true
      },
      "cell_type": "code",
      "source": "sns.set(style='white', context='notebook', palette='deep')\nwarnings.filterwarnings('ignore')\nsns.set_style('white')\n%matplotlib inline",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2f9bb958b20b3412b3497ed7f34d1f29f73c9c04"
      },
      "cell_type": "markdown",
      "source": "## 1-4 Data set"
    },
    {
      "metadata": {
        "_uuid": "cedecea930b278f86292367cc28d2996a235a169",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_df = pd.read_csv('../input/train.tsv', sep=\"\\t\")\ntest_df = pd.read_csv('../input/test.tsv', sep=\"\\t\")\nsub_df = pd.read_csv('../input/sampleSubmission.csv', sep=\",\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b6f8c6a6a9f29fc49ff2bb1d8294426c33563433"
      },
      "cell_type": "code",
      "source": "train_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6bb8c7baeb7f44bac5b466ba7623fbe17fb3f92b"
      },
      "cell_type": "code",
      "source": "print(\"Shape of train set                 : \",train_df.shape)\nprint(\"Shape of test set                  : \",test_df.shape)\nprint(\"Shape of sampleSubmission  : \",sub_df.shape)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f8add9178a5c9effd42f468319ab78a84c793d6d"
      },
      "cell_type": "code",
      "source": "train_df.columns",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "00dbaecad77715a366e1c16aa1905898efa6e249"
      },
      "cell_type": "code",
      "source": "print(train_df.info())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "45c537518846b1d52e7c9f6012210c085d5d6bd3"
      },
      "cell_type": "markdown",
      "source": "## Top 3 NLP Libraries Tutorial"
    },
    {
      "metadata": {
        "_uuid": "357b0262fecf7e24e7c8d9b671c4c695569e360f"
      },
      "cell_type": "markdown",
      "source": "1. NLTK\n1. spaCy\n1. Gensim"
    },
    {
      "metadata": {
        "_uuid": "c5048b61a4837c8826551c8871609973ebbe3847"
      },
      "cell_type": "markdown",
      "source": "<a id=\"38\"></a> <br>\n# 2- NLTK\nThe Natural Language Toolkit (NLTK) is one of the leading platforms for working with human language data and Python, the module NLTK is used for natural language processing. NLTK is literally an acronym for Natural Language Toolkit. with it you can tokenizing words and sentences.\nNLTK is a library of Python that can mine (scrap and upload data) and analyse very large amounts of textual data using computational methods.\n<img src='https://arts.unimelb.edu.au/__data/assets/image/0005/2735348/nltk.jpg'>"
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "adadeb7a83d0bc711a779948197c40841b10f1ca",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from nltk.tokenize import sent_tokenize, word_tokenize\n \ndata = \"All work and no play makes jack a dull boy, all work and no play\"\nprint(word_tokenize(data))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e49407d9fe96b86c9851fbd7065ebfb218281687"
      },
      "cell_type": "markdown",
      "source": "<a id=\"39\"></a> <br>\nAll of them are words except the comma. Special characters are treated as separate tokens.\n\n## 2-1 Tokenizing sentences\nThe same principle can be applied to sentences. Simply change the to sent_tokenize()\nWe have added two sentences to the variable data:"
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "ec9e6c715a1d49b2813c934fb4405ccec77884a1",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from nltk.tokenize import sent_tokenize, word_tokenize\n \ndata = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\"\nprint(sent_tokenize(data))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "75bb691fdb4982097ee8eb59ae930c1d81074afa"
      },
      "cell_type": "markdown",
      "source": "<a id=\"40\"></a> <br>\n## 2-2 NLTK and arrays\nIf you wish to you can store the words and sentences in arrays"
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "fde02d4189b0a52b7f919ac0fa0643d84ebacaf7",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from nltk.tokenize import sent_tokenize, word_tokenize\n \ndata = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\"\n \nphrases = sent_tokenize(data)\nwords = word_tokenize(data)\n \nprint(phrases)\nprint(words)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f7fad127cb8ed99cc063b98b3391645263737958"
      },
      "cell_type": "markdown",
      "source": "<a id=\"41\"></a> <br>\n## 2-3 NLTK stop words\nNatural language processing (nlp) is a research field that presents many challenges such as natural language understanding.\nText may contain stop words like ‘the’, ‘is’, ‘are’. Stop words can be filtered from the text to be processed. There is no universal list of stop words in nlp research, however the nltk module contains a list of stop words.\n\nIn this article you will learn how to remove stop words with the nltk module."
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "3357ec158943478d584c392bb7702fe7e6d4b355",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\n \ndata = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\"\nstopWords = set(stopwords.words('english'))\nwords = word_tokenize(data)\nwordsFiltered = []\n \nfor w in words:\n    if w not in stopWords:\n        wordsFiltered.append(w)\n \nprint(wordsFiltered)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "581a7ba2ce1ae5dae6c36d54f8999af838c7b80c"
      },
      "cell_type": "markdown",
      "source": "A module has been imported:\n\n"
    },
    {
      "metadata": {
        "_uuid": "2cb63648a5f138fe779744f0c52d570f30f84b13",
        "_kg_hide-input": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "from nltk.corpus import stopwords\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e7528080723ea540729e78b6e135475a870a5618"
      },
      "cell_type": "markdown",
      "source": "We get a set of English stop words using the line:\n\n"
    },
    {
      "metadata": {
        "_uuid": "6fbe468728072fb1883e064d0c1e892259fb1c0c",
        "_kg_hide-input": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "stopWords = set(stopwords.words('english'))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "43843cdaccbe961422631c13d982e13bf25607c6"
      },
      "cell_type": "markdown",
      "source": "The returned list stopWords contains 153 stop words on my computer.\nYou can view the length or contents of this array with the lines:"
    },
    {
      "metadata": {
        "_uuid": "53582f8f5ae2871e2cbba4542fc38965b61a5012",
        "_kg_hide-input": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(len(stopWords))\nprint(stopWords)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d25fed9ed1fd0016cea56de8e71b010a0d3176c3"
      },
      "cell_type": "markdown",
      "source": "We create a new list called wordsFiltered which contains all words which are not stop words.\nTo create it we iterate over the list of words and only add it if its not in the stopWords list."
    },
    {
      "metadata": {
        "_uuid": "3b28823a9862bb263183c6d62a4e91286bfe8d30",
        "_kg_hide-input": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "for w in words:\n    if w not in stopWords:\n        wordsFiltered.append(w)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8546a903a5b3916bca4feb96116dd00db1fc51c0"
      },
      "cell_type": "markdown",
      "source": "<a id=\"42\"></a> <br>\n## 2-4 NLTK – stemming\nStart by defining some words:"
    },
    {
      "metadata": {
        "_uuid": "f3a8e1427f235183fc5b9656a15a1cdb9befb55b",
        "_kg_hide-input": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "words = [\"game\",\"gaming\",\"gamed\",\"games\"]\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4a6ea3737dbb032d08c896f24cc6555b7b516274",
        "_kg_hide-input": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "from nltk.stem import PorterStemmer\nfrom nltk.tokenize import sent_tokenize, word_tokenize",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4bd4f0bc00227a77cfe734575ebfa7d124d333d4"
      },
      "cell_type": "markdown",
      "source": "And stem the words in the list using:"
    },
    {
      "metadata": {
        "_uuid": "12b5f99ca390ed21e68862e5eb5968d31e3858ef",
        "_kg_hide-input": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "from nltk.stem import PorterStemmer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\n\nwords = [\"game\",\"gaming\",\"gamed\",\"games\"]\nps = PorterStemmer()\n \nfor word in words:\n    print(ps.stem(word))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b0af11da2319b2be2643ad0003ebbc207067dc34"
      },
      "cell_type": "markdown",
      "source": "<a id=\"43\"></a> <br>\n## 2-5 NLTK speech tagging\nThe module NLTK can automatically tag speech.\nGiven a sentence or paragraph, it can label words such as verbs, nouns and so on.\n\nNLTK – speech tagging example\nThe example below automatically tags words with a corresponding class."
    },
    {
      "metadata": {
        "_uuid": "fe586131bec724c1901a441b56753c1d47562483",
        "_kg_hide-input": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "import nltk\nfrom nltk.tokenize import PunktSentenceTokenizer\n \ndocument = 'Whether you\\'re new to programming or an experienced developer, it\\'s easy to learn and use Python.'\nsentences = nltk.sent_tokenize(document)   \nfor sent in sentences:\n    print(nltk.pos_tag(nltk.word_tokenize(sent)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "22e469c3b880de5c4f00a609192e74d97d22436a"
      },
      "cell_type": "markdown",
      "source": "We can filter this data based on the type of word:"
    },
    {
      "metadata": {
        "_uuid": "8edca0d46e25f4d8dba85c454bb70299b7c1e112",
        "_kg_hide-input": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "import nltk\nfrom nltk.corpus import state_union\nfrom nltk.tokenize import PunktSentenceTokenizer\n \ndocument = 'Today the Netherlands celebrates King\\'s Day. To honor this tradition, the Dutch embassy in San Francisco invited me to'\nsentences = nltk.sent_tokenize(document)   \n \ndata = []\nfor sent in sentences:\n    data = data + nltk.pos_tag(nltk.word_tokenize(sent))\n \nfor word in data: \n    if 'NNP' in word[1]: \n        print(word)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "488b9eadaebb4fb6ae4585e991b9d1d6af176490"
      },
      "cell_type": "markdown",
      "source": "<a id=\"44\"></a> <br>\n## 2-6 Natural Language Processing – prediction\nWe can use natural language processing to make predictions. Example: Given a product review, a computer can predict if its positive or negative based on the text. In this article you will learn how to make a prediction program based on natural language processing."
    },
    {
      "metadata": {
        "_uuid": "a6dd913ea82ac7dcc478ab861e92e483a57beca0"
      },
      "cell_type": "markdown",
      "source": "<a id=\"45\"></a> <br>\n### 2-6-1 nlp prediction example\nGiven a name, the classifier will predict if it’s a male or female.\n\nTo create our analysis program, we have several steps:\n\n1. Data preparation\n1. Feature extraction\n1. Training\n1. Prediction\n1. Data preparation\nThe first step is to prepare data. We use the names set included with nltk."
    },
    {
      "metadata": {
        "_uuid": "73da028c3418b256653ec37d516b667af5541225",
        "_kg_hide-input": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "from nltk.corpus import names\n \n# Load data and training \nnames = ([(name, 'male') for name in names.words('male.txt')] + \n\t [(name, 'female') for name in names.words('female.txt')])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3421ce619481bc3f4b8817fd14c3c33374975c9b"
      },
      "cell_type": "markdown",
      "source": "This dataset is simply a collection of tuples. To give you an idea of what the dataset looks like:"
    },
    {
      "metadata": {
        "_uuid": "fd30af0cc73300d0d83c7d525af3281bfc718e54",
        "_kg_hide-input": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "[(u'Aaron', 'male'), (u'Abbey', 'male'), (u'Abbie', 'male')]\n[(u'Zorana', 'female'), (u'Zorina', 'female'), (u'Zorine', 'female')]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "022d450b9d514737392698136ee046fbbd73597c"
      },
      "cell_type": "markdown",
      "source": "You can define your own set of tuples if you wish, its simply a list containing many tuples.\n\nFeature extraction\nBased on the dataset, we prepare our feature. The feature we will use is the last letter of a name:\nWe define a featureset using:"
    },
    {
      "metadata": {
        "_uuid": "3745234017dc235c7e903943d2d67930402eac1a"
      },
      "cell_type": "markdown",
      "source": "featuresets = [(gender_features(n), g) for (n,g) in names]\nand the features (last letters) are extracted using:"
    },
    {
      "metadata": {
        "_uuid": "f3baaa6a29bbf22a153094df0bbf38c8a0ff0430",
        "_kg_hide-input": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "def gender_features(word): \n    return {'last_letter': word[-1]}",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9ef5cb06006e20577878cb93d2c8c488fe341c70"
      },
      "cell_type": "markdown",
      "source": "Training and prediction\nWe train and predict using:"
    },
    {
      "metadata": {
        "_uuid": "dba616c9612c65a875319837061d8c3861e9e6dd",
        "_kg_hide-input": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "import nltk.classify.util\nfrom nltk.classify import NaiveBayesClassifier\nfrom nltk.corpus import names\n \ndef gender_features(word): \n    return {'last_letter': word[-1]} \n \n# Load data and training \nnames = ([(name, 'male') for name in names.words('male.txt')] + \n\t [(name, 'female') for name in names.words('female.txt')])\n \nfeaturesets = [(gender_features(n), g) for (n,g) in names] \ntrain_set = featuresets\nclassifier = nltk.NaiveBayesClassifier.train(train_set) \n \n# Predict\nprint(classifier.classify(gender_features('Frank')))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d2d2be49b9597ad4fee4f0bc48dfd1df7934a447"
      },
      "cell_type": "markdown",
      "source": "If you want to give the name during runtime, change the last line to:\n\n"
    },
    {
      "metadata": {
        "_uuid": "a55ec4316a7dd340d09b4c93f579357b2022db5f",
        "_kg_hide-input": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Predict, you can change name\nname = 'Sarah'\nprint(classifier.classify(gender_features(name)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9f7e32d6aeeed974e9c9d23189241aa17ca69de3"
      },
      "cell_type": "markdown",
      "source": "<a id=\"45\"></a> <br>\n## 2-7 Python Sentiment Analysis\nIn Natural Language Processing there is a concept known as Sentiment Analysis.\n\nGiven a movie review or a tweet, it can be automatically classified in categories.\nThese categories can be user defined (positive, negative) or whichever classes you want.\nClassification is done using several steps: training and prediction.\n\nThe training phase needs to have training data, this is example data in which we define examples. The classifier will use the training data to make predictions."
    },
    {
      "metadata": {
        "_uuid": "180a352f2870cf979345790226b4d4bfe813c886"
      },
      "cell_type": "markdown",
      "source": "We start by defining 3 classes: positive, negative and neutral.\nEach of these is defined by a vocabulary:"
    },
    {
      "metadata": {
        "_uuid": "199b8ebaa48d358cf2a2520d4ee1b8edcf22f81c",
        "_kg_hide-input": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "positive_vocab = [ 'awesome', 'outstanding', 'fantastic', 'terrific', 'good', 'nice', 'great', ':)' ]\nnegative_vocab = [ 'bad', 'terrible','useless', 'hate', ':(' ]\nneutral_vocab = [ 'movie','the','sound','was','is','actors','did','know','words','not' ]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f50e153d90eb5519f6c76e487ac7c0212c9dd0ee"
      },
      "cell_type": "markdown",
      "source": "Every word is converted into a feature using a simplified bag of words model:"
    },
    {
      "metadata": {
        "_uuid": "5bba07df26463ff90b6d71a551f8aae0960de7e0",
        "_kg_hide-input": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "def word_feats(words):\n    return dict([(word, True) for word in words])\n \npositive_features = [(word_feats(pos), 'pos') for pos in positive_vocab]\nnegative_features = [(word_feats(neg), 'neg') for neg in negative_vocab]\nneutral_features = [(word_feats(neu), 'neu') for neu in neutral_vocab]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2e02426d80adacc804fc5497c9e3d7607e8cc030"
      },
      "cell_type": "markdown",
      "source": "Our training set is then the sum of these three feature sets:"
    },
    {
      "metadata": {
        "_uuid": "c9f2545fa20caca3034dbb62521e064ff4d322b6",
        "_kg_hide-input": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_set = negative_features + positive_features + neutral_features",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cf69fcf2e05e3a55c05c1f62adaa74c8cebfcf45"
      },
      "cell_type": "markdown",
      "source": "We train the classifier:"
    },
    {
      "metadata": {
        "_uuid": "0577b4e7dcb13b48a0918c7b632716482e9c042e"
      },
      "cell_type": "markdown",
      "source": "classifier = NaiveBayesClassifier.train(train_set)"
    },
    {
      "metadata": {
        "_uuid": "97abd270ce5492d1d178858fc1b7f2ef3eb71c69"
      },
      "cell_type": "markdown",
      "source": "This example classifies sentences according to the training set."
    },
    {
      "metadata": {
        "_uuid": "6214a67beb47748105aa7f0c51201925f39e9e7b",
        "_kg_hide-input": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "import nltk.classify.util\nfrom nltk.classify import NaiveBayesClassifier\nfrom nltk.corpus import names\n \ndef word_feats(words):\n    return dict([(word, True) for word in words])\n \npositive_vocab = [ 'awesome', 'outstanding', 'fantastic', 'terrific', 'good', 'nice', 'great', ':)' ]\nnegative_vocab = [ 'bad', 'terrible','useless', 'hate', ':(' ]\nneutral_vocab = [ 'movie','the','sound','was','is','actors','did','know','words','not' ]\n \npositive_features = [(word_feats(pos), 'pos') for pos in positive_vocab]\nnegative_features = [(word_feats(neg), 'neg') for neg in negative_vocab]\nneutral_features = [(word_feats(neu), 'neu') for neu in neutral_vocab]\n \ntrain_set = negative_features + positive_features + neutral_features\n \nclassifier = NaiveBayesClassifier.train(train_set) \n \n# Predict\nneg = 0\npos = 0\nsentence = \"Awesome movie, I liked it\"\nsentence = sentence.lower()\nwords = sentence.split(' ')\nfor word in words:\n    classResult = classifier.classify( word_feats(word))\n    if classResult == 'neg':\n        neg = neg + 1\n    if classResult == 'pos':\n        pos = pos + 1\n \nprint('Positive: ' + str(float(pos)/len(words)))\nprint('Negative: ' + str(float(neg)/len(words)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fe3d19ff691fcc7c7edf8d2cb1224e3bdeee396e"
      },
      "cell_type": "markdown",
      "source": "<a id=\"5\"></a> <br>\n# 5- conclusion\nAfter the first version of this kernel, in the third edition, we introduced NLTK library. in addition,  we examined each one in detail. this kernel it is not completed yet! Following up!"
    },
    {
      "metadata": {
        "_uuid": "a8424e6f84874112757040d36b93542a2e5ba8cb"
      },
      "cell_type": "markdown",
      "source": ">###### you may  be interested have a look at it: [**10-steps-to-become-a-data-scientist**](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist)\n\n\n---------------------------------------------------------------------\nyou can Fork and Run this kernel on Github:\n> ###### [ GitHub](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist)\n\n-------------------------------------------------------------------------------------------------------------\n\n **I hope you find this kernel helpful and some <font color=\"red\"><b>UPVOTES</b></font> would be very much appreciated**\n \n -----------"
    },
    {
      "metadata": {
        "_uuid": "1923ba01df86012077df2a2750b92ebb2adb8236"
      },
      "cell_type": "markdown",
      "source": "<a id=\"6\"></a> <br>\n# 6- References & Credits\n1. [Coursera](https://www.coursera.org/specializations/data-science-python)\n1. [GitHub](https://github.com/mjbahmani)\n1. [pythonspot](https://pythonspot.com/category/nltk/)\n1. [sunscrapers](https://sunscrapers.com/blog/6-best-python-natural-language-processing-nlp-libraries/)\n###### [Go to top](#top)"
    },
    {
      "metadata": {
        "_uuid": "7f0644ae4e74da4a20cba4e9094ed2458be44361"
      },
      "cell_type": "markdown",
      "source": "## This Kernel is not completed and will be updated soon!!!"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}